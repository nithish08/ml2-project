{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.logging import TensorBoardLogger\n",
    "import os\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_path = Path('./data')\n",
    "\n",
    "train = pd.read_csv(data_path/'train.csv')\n",
    "\n",
    "test = pd.read_csv(data_path / 'data_for_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1348470\n",
      "1348470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/nithish/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_enc = LabelEncoder()\n",
    "\n",
    "train = train.sort_values(['patient_id', 'key'])\n",
    "\n",
    "features =  ['gender', 'age', 'x1', 'x2', 'x3', 'x4', 'x5',\n",
    "       'x6', 'xx1', 'xx2','xx3','xx4','xx5']\n",
    "\n",
    "cat_features = ['x3']\n",
    "\n",
    "cont_features = [i for i in features if i not in cat_features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train[cont_features] = scaler.fit_transform(train[cont_features].values)\n",
    "\n",
    "train[cat_features] = label_enc.fit_transform(train[cat_features].values)\n",
    "\n",
    "train.x3.unique()\n",
    "\n",
    "val_patients = np.random.choice(train.patient_id.unique(), size=500, replace=False)\n",
    "\n",
    "print(train.shape[0])\n",
    "val = train[train.patient_id.isin(val_patients)]\n",
    "train = train[~train.patient_id.isin(val_patients)]\n",
    "print(val.shape[0]+ train.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class HeartDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, ts_features, cont_features, cat_features, target_columns=['y_mean_MAP', 'y_mean_HR']):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.ts_features = ts_features\n",
    "        self.cont_features = cont_features\n",
    "        self.cat_features = cat_features\n",
    "        self.target_columns = target_columns\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataframe.key.nunique()\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        timeseries = self.dataframe.iloc[30*idx : 30*(idx+1)][self.ts_features].values\n",
    "        timeseries = timeseries.T  # C_in x 30 (timestamps)\n",
    "        \n",
    "        cont_features = self.dataframe.loc[30*idx,self.cont_features].values\n",
    "        cat_features = self.dataframe.loc[30*idx,self.cat_features].values\n",
    "        \n",
    "        if self.target_columns:\n",
    "            target = self.dataframe.iloc[30*idx][self.target_columns].values\n",
    "            return [timeseries, cont_features, cat_features, target]\n",
    "        else:\n",
    "            return [timeseries, cont_features, cat_features]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ts_features = ['xx1', 'xx2', 'xx3', 'xx4', 'xx5']\n",
    "cat_features = ['x3']\n",
    "\n",
    "cont_features = ['gender', 'age', 'x1', 'x2', 'x4', 'x5', 'x6']\n",
    "\n",
    "target_columns = ['y_mean_MAP', 'y_mean_HR']\n",
    "\n",
    "\n",
    "dataset_train = HeartDataset(train,ts_features, cont_features, cat_features, target_columns)\n",
    "dataset_val = HeartDataset(val,ts_features, cont_features,cat_features , target_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "nn.MultiheadAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    ts = np.stack([item[0] for item in batch])\n",
    "    cont_features = np.stack([item[1] for item in batch])\n",
    "    cat_features = np.stack([item[2] for item in batch])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ts = torch.from_numpy(ts).float()\n",
    "    cont_features = torch.from_numpy(cont_features.astype(float)).float()\n",
    "    cat_features = torch.from_numpy(cat_features.astype(float)).long()\n",
    "    \n",
    "    \n",
    "    # use gpu\n",
    "    \n",
    "    ts = ts.to(device)\n",
    "    cont_features = cont_features.to(device)\n",
    "    cat_features = cat_features.to(device)\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        target = np.stack([item[3] for item in batch])\n",
    "        target = torch.from_numpy(target.astype(float)).float()\n",
    "        target = target.to(device)\n",
    "        return [ts, cont_features, cat_features, target]\n",
    "\n",
    "    except:\n",
    "        return [ts, cont_features, cat_features]\n",
    "\n",
    "def r2_squared_loss_function(preds , target):\n",
    "    '''\n",
    "    PREDICTIONS --> TARGET ORDER IMPORTANT\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    mean_target = torch.mean(target,0)\n",
    "\n",
    "    mean_mse = torch.mean((target-mean_target)**2,0)\n",
    "    pred_mse = torch.mean((preds-target)**2,0)\n",
    "    \n",
    "    div = 1 - pred_mse/(mean_mse+10e-6)\n",
    "    \n",
    "    mean_r2_squared = torch.mean(div)\n",
    "    \n",
    "    return -mean_r2_squared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# batch_size =  128\n",
    "# dataloader_train = DataLoader(dataset_train, batch_size=batch_size,collate_fn=custom_collate)\n",
    "# dataloader_val = DataLoader(dataset_val, batch_size=batch_size,collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CoolSystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        super(CoolSystem, self).__init__()\n",
    "        # not the best model...\n",
    "        self.fc1 = nn.Linear(167,64)\n",
    "        self.fc2 = nn.Linear(64,2)\n",
    "        self.emb = nn.Embedding(5, 10)\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, ts, cont, cat):\n",
    "        ts = self.flat1(ts)\n",
    "        cat = self.emb(cat).squeeze(1)\n",
    "        merged = torch.cat([ts,cont,cat],axis=1)\n",
    "        merged = self.fc1(merged)\n",
    "        merged = F.relu(merged)\n",
    "        merged = self.fc2(merged)\n",
    "#         merged = F.relu(merged)\n",
    "#         merged = self.fc3(merged)\n",
    "        \n",
    "        return merged\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # REQUIRED\n",
    "        ts, cont, cat, y = batch\n",
    "        y_hat = self.forward(ts, cont, cat)\n",
    "        loss = r2_squared_loss_function(y_hat, y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # OPTIONAL\n",
    "        ts, cont, cat, y = batch\n",
    "        y_hat = self.forward(ts, cont, cat)\n",
    "        loss = r2_squared_loss_function(y_hat, y)\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        # (LBFGS it is automatically supported, no need for closure function)\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.03)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED\n",
    "        return DataLoader(dataset_train, batch_size=self.batch_size,\n",
    "                          collate_fn=custom_collate,shuffle=True)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(dataset_val, batch_size=self.batch_size,\n",
    "                          collate_fn=custom_collate, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# dataloader_train = DataLoader(dataset_train, batch_size=32,\n",
    "#                           collate_fn=custom_collate,shuffle=True)\n",
    "# batch = next(iter(dataloader_train))\n",
    "\n",
    "# ts, cont, cat, y = batch\n",
    "\n",
    "# emb_layer = nn.Embedding(5, 10)\n",
    "\n",
    "# cat = emb_layer(cat).squeeze(1)\n",
    "\n",
    "# cat.shape\n",
    "\n",
    "# ts.shape, cont.shape, cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# gru1 = nn.GRU(input_size=5, hidden_size=64, batch_first=True,\n",
    "#              num_layers=1)\n",
    "\n",
    "\n",
    "\n",
    "# gru2 = nn.GRU(input_size=64, hidden_size=32, batch_first=True,\n",
    "#              num_layers=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SimpleGRU(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        \n",
    "        # batch_size\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        #emb layer \n",
    "        self.emb = nn.Embedding(5,10)\n",
    "        # layers\n",
    "        # conv layers\n",
    "        self.gru1 = nn.GRU(input_size=5, hidden_size=64, batch_first=True,\n",
    "             num_layers=3)\n",
    " \n",
    "        self.gru2 = nn.GRU(input_size=64, hidden_size=32, batch_first=True,\n",
    "             num_layers=1)\n",
    "\n",
    "        self.relu_merged = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=49, out_features=32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(in_features=16, out_features=2)\n",
    "\n",
    "    def forward(self, ts, cont, cat):\n",
    "        \n",
    "        # ts into GRU\n",
    "        o1,o2 = self.gru1(ts.transpose(1,2))\n",
    "        o3,o4 = self.gru2(o1)\n",
    "        o4 = o4.squeeze(0)\n",
    "\n",
    "        # remove cat dimen\n",
    "        cat = self.emb(cat).squeeze(1)\n",
    "        \n",
    "        # merge after embedding\n",
    "        merged = torch.cat([o4,cont,cat],axis=1)\n",
    "        merged = self.relu_merged(merged)\n",
    "        \n",
    "        merged = self.fc1(merged)\n",
    "        merged = self.relu1(merged)\n",
    "        merged = self.fc2(merged)\n",
    "        merged = self.relu2(merged)\n",
    "        merged = self.fc3(merged)\n",
    "\n",
    "        \n",
    "        return merged\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # REQUIRED\n",
    "        ts, cont, cat, y = batch\n",
    "        y_hat = self.forward(ts, cont, cat)\n",
    "        loss = r2_squared_loss_function(y_hat, y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # OPTIONAL\n",
    "        ts, cont, cat, y = batch\n",
    "        y_hat = self.forward(ts, cont, cat)\n",
    "        loss = r2_squared_loss_function(y_hat, y)\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        # (LBFGS it is automatically supported, no need for closure function)\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED\n",
    "        return DataLoader(dataset_train, batch_size=self.hparams.batch_size,\n",
    "                          collate_fn=custom_collate,shuffle=True)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(dataset_val, batch_size=self.hparams.batch_size,\n",
    "                          collate_fn=custom_collate, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "          Name       Type Params\n",
      "0          emb  Embedding   50  \n",
      "1         gru1        GRU   63 K\n",
      "2         gru2        GRU    9 K\n",
      "3  relu_merged       ReLU    0  \n",
      "4          fc1     Linear    1 K\n",
      "5        relu1       ReLU    0  \n",
      "6          fc2     Linear  528  \n",
      "7        relu2       ReLU    0  \n",
      "8          fc3     Linear   34  \n",
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/12 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  75%|███████▌  | 9/12 [01:24<00:25,  8.41s/batch, batch_idx=8, loss=17.884, v_num=0]\n",
      "Validating:   0%|          | 0/3 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 10/12 [01:34<00:17,  8.88s/batch, batch_idx=8, loss=17.884, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 11/12 [01:44<00:09,  9.23s/batch, batch_idx=8, loss=17.884, v_num=0]\n",
      "Epoch 1: 100%|██████████| 12/12 [01:49<00:00,  7.96s/batch, batch_idx=8, loss=17.884, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 9/12 [01:24<00:24,  8.32s/batch, batch_idx=8, loss=14.975, v_num=0] \n",
      "Validating:   0%|          | 0/3 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 2:  83%|████████▎ | 10/12 [01:34<00:17,  8.86s/batch, batch_idx=8, loss=14.975, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 11/12 [01:44<00:09,  9.19s/batch, batch_idx=8, loss=14.975, v_num=0]\n",
      "Epoch 2: 100%|██████████| 12/12 [01:49<00:00,  7.94s/batch, batch_idx=8, loss=14.975, v_num=0]\n",
      "Epoch 3:  75%|███████▌  | 9/12 [01:25<00:25,  8.40s/batch, batch_idx=8, loss=11.354, v_num=0] \n",
      "Validating:   0%|          | 0/3 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 3:  83%|████████▎ | 10/12 [01:35<00:17,  8.89s/batch, batch_idx=8, loss=11.354, v_num=0]\n",
      "Epoch 3:  92%|█████████▏| 11/12 [01:45<00:09,  9.24s/batch, batch_idx=8, loss=11.354, v_num=0]\n",
      "Epoch 3: 100%|██████████| 12/12 [01:50<00:00,  7.99s/batch, batch_idx=8, loss=11.354, v_num=0]\n",
      "Epoch 4:  75%|███████▌  | 9/12 [01:25<00:25,  8.37s/batch, batch_idx=8, loss=9.026, v_num=0]  \n",
      "Validating:   0%|          | 0/3 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 4:  83%|████████▎ | 10/12 [01:35<00:17,  8.86s/batch, batch_idx=8, loss=9.026, v_num=0]\n",
      "Epoch 4:  92%|█████████▏| 11/12 [01:45<00:09,  9.20s/batch, batch_idx=8, loss=9.026, v_num=0]\n",
      "Epoch 4: 100%|██████████| 12/12 [01:50<00:00,  7.98s/batch, batch_idx=8, loss=9.026, v_num=0]\n",
      "Epoch 6:  75%|███████▌  | 9/12 [01:24<00:24,  8.27s/batch, batch_idx=8, loss=6.364, v_num=0] \n",
      "Validating:   0%|          | 0/3 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 6:  83%|████████▎ | 10/12 [01:34<00:17,  8.74s/batch, batch_idx=8, loss=6.364, v_num=0]\n",
      "Epoch 6:  92%|█████████▏| 11/12 [01:44<00:09,  9.07s/batch, batch_idx=8, loss=6.364, v_num=0]\n",
      "Epoch 6: 100%|██████████| 12/12 [01:48<00:00,  7.84s/batch, batch_idx=8, loss=6.364, v_num=0]\n",
      "Epoch 7:  75%|███████▌  | 9/12 [01:24<00:24,  8.33s/batch, batch_idx=8, loss=5.521, v_num=0] \n",
      "Validating:   0%|          | 0/3 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 7:  83%|████████▎ | 10/12 [01:34<00:17,  8.83s/batch, batch_idx=8, loss=5.521, v_num=0]\n",
      "Epoch 7:  92%|█████████▏| 11/12 [01:44<00:09,  9.19s/batch, batch_idx=8, loss=5.521, v_num=0]\n",
      "Epoch 7: 100%|██████████| 12/12 [01:49<00:00,  7.94s/batch, batch_idx=8, loss=5.521, v_num=0]\n",
      "Epoch 8:  75%|███████▌  | 9/12 [01:24<00:24,  8.24s/batch, batch_idx=8, loss=4.860, v_num=0] \n",
      "Validating:   0%|          | 0/3 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 8:  83%|████████▎ | 10/12 [01:34<00:17,  8.74s/batch, batch_idx=8, loss=4.860, v_num=0]\n",
      "Epoch 8:  92%|█████████▏| 11/12 [01:43<00:09,  9.08s/batch, batch_idx=8, loss=4.860, v_num=0]\n",
      "Epoch 8: 100%|██████████| 12/12 [01:48<00:00,  7.86s/batch, batch_idx=8, loss=4.860, v_num=0]\n",
      "Epoch 9:  75%|███████▌  | 9/12 [01:26<00:25,  8.56s/batch, batch_idx=8, loss=4.330, v_num=0] \n",
      "Validating:   0%|          | 0/3 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 9:  83%|████████▎ | 10/12 [01:36<00:18,  9.13s/batch, batch_idx=8, loss=4.330, v_num=0]\n",
      "Epoch 9:  92%|█████████▏| 11/12 [01:47<00:09,  9.49s/batch, batch_idx=8, loss=4.330, v_num=0]\n",
      "Epoch 9: 100%|██████████| 12/12 [01:52<00:00,  8.19s/batch, batch_idx=8, loss=4.330, v_num=0]\n",
      "Epoch 10:  75%|███████▌  | 9/12 [01:28<00:26,  8.73s/batch, batch_idx=8, loss=3.899, v_num=0]\n",
      "Validating:   0%|          | 0/3 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 10:  83%|████████▎ | 10/12 [01:38<00:18,  9.30s/batch, batch_idx=8, loss=3.899, v_num=0]\n",
      "Epoch 10:  92%|█████████▏| 11/12 [01:49<00:09,  9.75s/batch, batch_idx=8, loss=3.899, v_num=0]\n",
      "Epoch 10: 100%|██████████| 12/12 [01:54<00:00,  8.44s/batch, batch_idx=8, loss=3.899, v_num=0]\n",
      "Epoch 11:  75%|███████▌  | 9/12 [01:30<00:26,  8.78s/batch, batch_idx=8, loss=3.543, v_num=0] \n",
      "Validating:   0%|          | 0/3 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 11:  83%|████████▎ | 10/12 [01:40<00:18,  9.25s/batch, batch_idx=8, loss=3.543, v_num=0]\n",
      "Epoch 11:  92%|█████████▏| 11/12 [01:50<00:09,  9.61s/batch, batch_idx=8, loss=3.543, v_num=0]\n",
      "Epoch 11: 100%|██████████| 12/12 [01:56<00:00,  8.30s/batch, batch_idx=8, loss=3.543, v_num=0]\n",
      "Epoch 12:  75%|███████▌  | 9/12 [01:28<00:26,  8.72s/batch, batch_idx=8, loss=2.079, v_num=0] \n",
      "Validating:   0%|          | 0/3 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 12:  83%|████████▎ | 10/12 [01:39<00:18,  9.27s/batch, batch_idx=8, loss=2.079, v_num=0]\n",
      "Epoch 12:  92%|█████████▏| 11/12 [01:49<00:09,  9.59s/batch, batch_idx=8, loss=2.079, v_num=0]\n",
      "Epoch 12: 100%|██████████| 12/12 [01:55<00:00,  8.32s/batch, batch_idx=8, loss=2.079, v_num=0]\n",
      "Epoch 13:  42%|████▏     | 5/12 [00:52<01:10, 10.08s/batch, batch_idx=4, loss=1.177, v_num=0] "
     ]
    }
   ],
   "source": [
    "name = 'gru3relus'\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "                save_dir='ts-logs',\n",
    "                name = name\n",
    "\n",
    "            )\n",
    "\n",
    "early_stopping = EarlyStopping('val_loss',patience=5)\n",
    "checkpoint_callback = ModelCheckpoint(filepath=f'saved_models/{name}')\n",
    "\n",
    "trainer = pl.Trainer(logger=logger,early_stop_callback=early_stopping,checkpoint_callback=checkpoint_callback)\n",
    "\n",
    "hparams = {'batch_size':4096,\n",
    "          'lr':0.1}\n",
    "\n",
    "hparams = Namespace(**hparams)\n",
    "\n",
    "simplegru = SimpleGRU(hparams)\n",
    "# model = model.cuda()\n",
    "\n",
    "simplegru = simplegru.to(device)\n",
    "\n",
    "trainer.fit(simplegru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SimpleCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # batch_size\n",
    "        self.hparams = hparams\n",
    "        self.batch_size = self.hparams.batch_size  # for dataloaders\n",
    "        \n",
    "        #emb layer \n",
    "        self.emb = nn.Embedding(5,10)\n",
    "        # layers\n",
    "        # conv layers\n",
    "        self.cnn1 = nn.Conv1d(in_channels=5, out_channels=128, kernel_size=3, dilation=1,)\n",
    "        self.relu1 = nn.ReLU()\n",
    "#         self.maxpool1 = nn.MaxPool1d(kernel_size=3)\n",
    "        \n",
    "        self.cnn2 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "#         self.maxpool2 = nn.MaxPool1d(kernel_size=3)\n",
    "        \n",
    "        self.cnn3 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "#         self.maxpool3 = nn.MaxPool1d(kernel_size=3)\n",
    "        \n",
    "        self.cnn_flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=785, out_features=32)\n",
    "        self.relu_combined = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=2)\n",
    "\n",
    "    def forward(self, ts, cont, cat):\n",
    "        ts = self.relu1(self.cnn1(ts)) # first conv\n",
    "        ts = self.relu2(self.cnn2(ts))\n",
    "        ts = self.relu3(self.cnn3(ts))\n",
    "        ts = self.cnn_flatten(ts)\n",
    "        \n",
    "        cat = self.emb(cat).squeeze(1)\n",
    "        merged = torch.cat([ts,cont,cat],axis=1)\n",
    "        merged = self.fc1(merged)\n",
    "        merged = self.relu_combined(merged)\n",
    "        merged = self.fc2(merged)\n",
    "#         merged = F.relu(merged)\n",
    "#         merged = self.fc3(merged)\n",
    "        \n",
    "        return merged\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # REQUIRED\n",
    "        ts, cont, cat, y = batch\n",
    "        y_hat = self.forward(ts, cont, cat)\n",
    "        loss = r2_squared_loss_function(y_hat, y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # OPTIONAL\n",
    "        ts, cont, cat, y = batch\n",
    "        y_hat = self.forward(ts, cont, cat)\n",
    "        loss = r2_squared_loss_function(y_hat, y)\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        # (LBFGS it is automatically supported, no need for closure function)\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED\n",
    "        return DataLoader(dataset_train, batch_size=self.batch_size,\n",
    "                          collate_fn=custom_collate,shuffle=True)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(dataset_val, batch_size=self.batch_size,\n",
    "                          collate_fn=custom_collate, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "name = 'no_maxpool'\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "                save_dir='ts-logs',\n",
    "                name = name\n",
    "\n",
    "            )\n",
    "\n",
    "early_stopping = EarlyStopping('val_loss',patience=5)\n",
    "checkpoint_callback = ModelCheckpoint(filepath=f'saved_models/{name}')\n",
    "\n",
    "trainer = pl.Trainer(logger=logger,early_stop_callback=early_stopping,checkpoint_callback=checkpoint_callback)\n",
    "\n",
    "hparams = {'batch_size':1024,\n",
    "          'lr':3e-3}\n",
    "\n",
    "hparams = Namespace(**hparams)\n",
    "\n",
    "simplecnn = SimpleCNN(hparams)\n",
    "# model = model.cuda()\n",
    "\n",
    "simplecnn = simplecnn.to(device)\n",
    "\n",
    "trainer.fit(simplecnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.8792, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "whole_val_metric(model, dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def whole_val_metric(model, dataset_val):\n",
    "    model.eval()\n",
    "    dl = DataLoader(dataset_val, batch_size=len(dataset_val),collate_fn=custom_collate)\n",
    "    ts, cont, cat, y = next(iter(dl))\n",
    "    y_hat = model(ts, cont, cat)\n",
    "    print(r2_squared_loss_function(y_hat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_num_parameters(model):\n",
    "    s = 0\n",
    "    for param in model.parameters():\n",
    "        s += param.numel()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Test predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
